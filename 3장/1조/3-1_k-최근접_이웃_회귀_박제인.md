# Chapter3. 회귀 알고리즘과 모델 규제
#### - 지도 학습 알고리즘의 한 종류인 회귀 알고리즘에 대해 공부
#### - 다양한 선형 회귀 알고리즘의 장단점 이해

<br>

### 회귀
- 지도 학습 알고리즘은 크게 분류와 회귀로 나뉜다.
- 분류는 샘플을 몇 개의 클래스 중 하나로 분류하는 문제이고 회귀는 클래스 중 하나로 분류하는 것이 아니라 임의의 어떤 숫자를 예측하는 것이다.
- 임의의 수치를 예측하는 문제, 타깃값도 임의의 수치가 된다

### k-최근접 이웃 회귀
k-최근접 이웃 알고리즘을 사용해 회귀 문제를 푼다. 가장 가까운 이웃 샘플을 찾고 이 샘플들의 타깃값의 평균을 구해 예측으로 삼는다.

### 결정계수(R<sup>2</sup>)
대표적인 회귀 문제의 성능 측정 도구이다. 
1에 가까울수록 성능이 좋고 0에 가까울수록 성능이 나쁜 모델이다.
R<sup>2</sup>= 1 - { (타깃-예측<sup>2</sup>의 합) / (타깃-평균<sup>2</sup>의 합) }
- 각 샘플의 타깃과 예측한 값의 차이를 제곱하여 더함
- 타깃과 타깃 평균의 차이를 제곱하여 더한 값으로 나눔
- 타깃의 평균 정도를 예측한다면 R<sup>2</sup>는 0에 가까워짐
- 예측이 타깃에 아주 가까워지면 1에 가까워짐

<br>

## 과대적합과 과소적합
- ### 과대적합
훈련 세트에만 잘 맞게 학습된 모델
모델의 훈련 세트 성능이 테스트 세트 성능보다 훨씬 높을 때 일어난다.
모델이 훈련세트에 너무 집착해서 데이터에 내재된 거시적인 패턴을 감지하지 못한다. (테스트 세트 혹은 실제 적용 시 잘 동작하지 않음)

- ### 과소적합
과대적합과 반대로 테스트 세트 성능이 훈련 세트 성능보다 더 높을 때 일어나고 훈련세트와 테스트 세트 성능이 모두 동일하게 낮을 때도 일어난다.
모델이 너무 단순하여 훈련 세트에 적절히 훈련되지 않은 경우이다.
훈련 세트/테스트 세트 크기가 매우 작은 경우에도 과소적합이 발생한다.

- ### 과대적합과 과소적합 해결
  - 모델을 더 복잡하게 만들어야 함
  - k-최근접 이웃 알고리즘에서는 이웃의 개수 k를 줄여 모델을 복잡하게 만듦
    - 이웃의 개수를 줄이면 훈련 세트에 있는 국지적 패턴에 민감해짐
    - 이웃의 개수를 늘리면 데이터 전반에 있는 일반적 패턴을 따름
